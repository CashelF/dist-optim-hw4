{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1449d575",
   "metadata": {
    "id": "1449d575"
   },
   "source": [
    "# Homework 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0766e641",
   "metadata": {
    "id": "0766e641"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad4b1eb",
   "metadata": {
    "id": "8ad4b1eb"
   },
   "source": [
    "**Note**: The experiments in this project can take some time to complete. On my computer, they take more than 30 minutes to run in total (across all experiments). Please be sure to leave enough time to complete the experimental exploration.\n",
    "\n",
    "**Also note**: Due to the environment variation (Linux/Windows/macOS/Colab), you may encounter some environment issues. Reach out to TA if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e2cc37",
   "metadata": {
    "id": "e1e2cc37"
   },
   "source": [
    "## Task 0: Setup the environment (0%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd99ae62",
   "metadata": {
    "id": "fd99ae62"
   },
   "source": [
    "You will complete this project using PyTorch. Install the Pytorch before you run this notebook.\n",
    "\n",
    "You can install the package by yourself, or you can simply run the following command to install. If you run your code in Colab, it is the most convenient way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N7Z2IauIrhH_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8075,
     "status": "ok",
     "timestamp": 1762812874620,
     "user": {
      "displayName": "Five Mr",
      "userId": "01981404490572427296"
     },
     "user_tz": 300
    },
    "id": "N7Z2IauIrhH_",
    "outputId": "4b241d4b-b57d-4461-ba8a-4cc46804e3a8"
   },
   "outputs": [],
   "source": [
    "! pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Uoum9kXxxQ9T",
   "metadata": {
    "id": "Uoum9kXxxQ9T"
   },
   "source": [
    "An alternative way to install the environment is provided as follows. After running them, a Conda environment will be ready. Don't forget to activate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86284f1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1762750726678,
     "user": {
      "displayName": "Yuheng Wang",
      "userId": "18233237358572767029"
     },
     "user_tz": 300
    },
    "id": "86284f1c",
    "outputId": "d7bad938-fa6a-435b-a531-a36027ddead6"
   },
   "outputs": [],
   "source": [
    "# If you use CPU, uncommand the following command and run the cell\n",
    "! conda env update -f environment-cpu.yml --prune\n",
    "# If you use GPU, uncommand the following command and run the cell\n",
    "# ! conda env update -f environment-gpu.yml --prune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b05bd",
   "metadata": {
    "id": "f36b05bd"
   },
   "source": [
    "Hint: Some code in this notebook will be cached dynamically. Rerun the following code every time you restart the notebook to ensure every functions work perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018712c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "error",
     "timestamp": 1762812917274,
     "user": {
      "displayName": "Five Mr",
      "userId": "01981404490572427296"
     },
     "user_tz": 300
    },
    "id": "018712c9",
    "outputId": "b5cc58aa-6dc4-4ddd-edfa-d48da3a628b0"
   },
   "outputs": [],
   "source": [
    "# Reload packages automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_dCdgoVDyJvq",
   "metadata": {
    "id": "_dCdgoVDyJvq"
   },
   "source": [
    "If the code above fail, you could try the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q6SGDZHuyCML",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5472,
     "status": "ok",
     "timestamp": 1762812906447,
     "user": {
      "displayName": "Five Mr",
      "userId": "01981404490572427296"
     },
     "user_tz": 300
    },
    "id": "Q6SGDZHuyCML",
    "outputId": "288f1e4f-745d-4b02-dfdb-8098e892946e"
   },
   "outputs": [],
   "source": [
    "!pip install ipython --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbdbc82",
   "metadata": {
    "id": "1fbdbc82",
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "## Task 1: Distributed operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cf286f",
   "metadata": {
    "id": "63cf286f"
   },
   "source": [
    "Before start, it is suggested to check the number of processors (CPUs or GPUs). In principle, more than 1 processor is sufficient. However, it is suggested to run and test the code in the platform with more than 4 processes, which enables us to observe the benefit of distributed computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d394f",
   "metadata": {
    "executionInfo": {
     "elapsed": 6566,
     "status": "ok",
     "timestamp": 1762813086478,
     "user": {
      "displayName": "Five Mr",
      "userId": "01981404490572427296"
     },
     "user_tz": 300
    },
    "id": "2b3d394f"
   },
   "outputs": [],
   "source": [
    "USE_GPU = False  # If you use CPU\n",
    "# USE_GPU = True  # If you use GPU\n",
    "\n",
    "if USE_GPU:\n",
    "    import torch\n",
    "    print(\"Number of GPUs: \", torch.cuda.device_count())\n",
    "    backend = \"nccl\"\n",
    "else:\n",
    "    import torch.multiprocessing as mp\n",
    "    backend = \"gloo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd633ed1",
   "metadata": {
    "id": "fd633ed1"
   },
   "source": [
    "### Step 1: Distributed \"Hello World\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d655c",
   "metadata": {
    "id": "8e3d655c"
   },
   "source": [
    "In distributed computing, two fundamental concepts are ``world size`` and ``rank``, which provide the necessary context for managing data, synchronizing model updates, and orchestrating the entire distributed training workflow.\n",
    "\n",
    "``World size`` refers to the total number of processes participating in a distributed computing job. This is the global count of all workers, which could be across multiple machines or multiple GPUs on a single machine.\n",
    "\n",
    "``Rank``, on the other hand, is the unique identifier assigned to each individual process within the distributed group. Each process has a distinct rank ranging from 0 to ``World size`` - 1. This unique identifier is crucial for coordinating communication and ensuring that each process knows its specific role in the training task, such as which partition of data or which subset of a model it is responsible for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8f7f2f",
   "metadata": {
    "id": "5a8f7f2f"
   },
   "source": [
    "First, we define necessary configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd124be",
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1762813841944,
     "user": {
      "displayName": "Five Mr",
      "userId": "01981404490572427296"
     },
     "user_tz": 300
    },
    "id": "bbd124be"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "world_size = 4\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '29501'\n",
    "\n",
    "os.makedirs('cache', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0660f3a",
   "metadata": {
    "id": "c0660f3a"
   },
   "source": [
    "Now we need to define a function which defines what should the each worker do.\n",
    "\n",
    "The first step is to let each worker print its rank and your name. For example, if you are Alice, your output should be like\n",
    "```bash\n",
    "Rank 1: Hello, Alice!\n",
    "Rank 2: Hello, Alice!\n",
    "Rank 3: Hello, Alice!\n",
    "Rank 4: Hello, Alice!\n",
    "```\n",
    "\n",
    "Please complete the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a3839",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1762813910780,
     "user": {
      "displayName": "Five Mr",
      "userId": "01981404490572427296"
     },
     "user_tz": 300
    },
    "id": "447a3839",
    "outputId": "4416ffd3-a089-46fd-93c2-08e8004d3ba8"
   },
   "outputs": [],
   "source": [
    "%%writefile cache/H4T1S1.py\n",
    "# No need to worry about this line. It's introduced to make thing go well in Jupyter notebook environment.\n",
    "# ================================================\n",
    "\n",
    "def say_hello(rank, results_queue):\n",
    "\n",
    "    \"\"\"\"Step 1: Distributed ''Hello World''\n",
    "    \"\"\"\n",
    "    # =========================================\n",
    "    # Answer:\n",
    "\n",
    "    # =========================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a898a2",
   "metadata": {
    "id": "15a898a2"
   },
   "source": [
    "We will use the ``spawn`` function provided by the Python ``multiprocessing`` package as the entry. ``multiprocessing`` is a powerful tool for leveraging the full potential of modern multi-core processors. The usage of ``spawn`` is\n",
    "```python\n",
    "import torch.multiprocessing as mp\n",
    "mp.spawn(func, args=(), nprocs=world_size, join=True)\n",
    "```\n",
    "where the arguments includes:\n",
    "\n",
    "``func``: function to be performed by each process. This function should take at least one argument, the rank of the process.\n",
    "\n",
    "``args``: A tuple of arguments to pass to the function. These arguments are shared across all processes.\n",
    "\n",
    "``nprocs``: The number of processes to spawn. In this case, it is set to the value of ``world_size``, which is 4.\n",
    "\n",
    "``join``: A boolean indicating whether the main process should wait for all spawned processes to finish before continuing. If set to ``True``, the main process will block until all processes have completed.\n",
    "\n",
    "\n",
    "Depending on the operating system, ``print`` executed inside subprocesses may not appear in the Jupyter Notebook output. To address this, we include an additional argument, ``results_queue``, in the ``say_hello`` function. Each subprocess can place its messages into this queue, and the main process can retrieve and print them after all subprocesses have completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42778af9",
   "metadata": {
    "id": "42778af9"
   },
   "source": [
    "Now we run the ``say_hello`` function on each workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aea4844",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12539,
     "status": "ok",
     "timestamp": 1762813988441,
     "user": {
      "displayName": "Five Mr",
      "userId": "01981404490572427296"
     },
     "user_tz": 300
    },
    "id": "9aea4844",
    "outputId": "145f6e8d-f941-4c19-dd97-27d0ace9c15b"
   },
   "outputs": [],
   "source": [
    "# No need to worry about this line. It's introduced to make thing go well in Jupyter notebook environment.\n",
    "from cache.H4T1S1 import say_hello\n",
    "# ================================================\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "results_queue = mp.Queue()\n",
    "\n",
    "# Use spawn to create separate processes for each rank\n",
    "mp.spawn(say_hello, args=(results_queue, ), nprocs=world_size, join=True)\n",
    "\n",
    "# read results from the subprocess\n",
    "for _ in range(world_size):\n",
    "    # get() will block until there is message in the queue\n",
    "    result = results_queue.get()\n",
    "    print(result)\n",
    "\n",
    "print(\"All processes finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cd8046",
   "metadata": {
    "id": "29cd8046"
   },
   "source": [
    "### Step 2: Communication among workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8fd23c",
   "metadata": {
    "id": "eb8fd23c"
   },
   "source": [
    "\n",
    "A random tensor $x_n$ is generated on each worker $n$ (already being provided). You need to implement a code that allows worker $n$ to send $x_n$ to worker $(n+1) \\bmod N$, where $N$ is the world size.\n",
    "\n",
    "Follow the hints in the function to complete the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511affcf",
   "metadata": {
    "id": "511affcf",
    "outputId": "872ad423-9b00-4fd3-b0b2-26fbcc57cacd"
   },
   "outputs": [],
   "source": [
    "%%writefile cache/H4T1S2.py\n",
    "# No need to worry about this line. It's introduced to make thing go well in Jupyter notebook environment.\n",
    "# ================================================\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "from util import test_func_H3T1S2, generate_random_tensor\n",
    "\n",
    "def send_message_to_next(rank, world_size, message_size, backend=\"gloo\"):\n",
    "\n",
    "    # =========================================================\n",
    "    #   Step 0: Initialize the process group.\n",
    "    # =========================================================\n",
    "\n",
    "    # A helper function to run the example.\n",
    "    dist.init_process_group(backend=backend, rank=rank, world_size=world_size)\n",
    "\n",
    "    # prepare the tensor to send\n",
    "    my_tensor = generate_random_tensor(rank, message_size)\n",
    "\n",
    "    # =========================================================\n",
    "    #   Step 1: Create a placeholder for the received tensor.\n",
    "    # =========================================================\n",
    "    # The receive function requires a pre-allocated tensor\n",
    "    # to place the incoming data. The size must match the data sent.\n",
    "    # ----------------------------------------------------------\n",
    "    # Answer:\n",
    "\n",
    "    # =========================================================\n",
    "    #   Step 2: Determine sender and receiver ranks.\n",
    "    # =========================================================\n",
    "    # The goal is for worker `n` to send to `(n+1) % N` and\n",
    "    # receive from `(n-1 + N) % N`. \n",
    "    # Your goal here is to find two values: `receiver_rank` and `source_rank`.\n",
    "    # receiver_rank: The rank of the worker we will send to.\n",
    "    # source_rank: The rank of the worker we will receive from.\n",
    "    # ----------------------------------------------------------\n",
    "    # Answer:\n",
    "\n",
    "\n",
    "\n",
    "    # =========================================================\n",
    "    #   Step 3: Perform the send and receive operations.\n",
    "    # =========================================================\n",
    "\n",
    "    # We use asynchronous operations to allow for simultaneous\n",
    "    # communication. This is more efficient than synchronous sends/receives.\n",
    "    # `dist.isend` initiates a non-blocking send.\n",
    "    # `dist.irecv` initiates a non-blocking receive.\n",
    "    # Your goal here is to create two request objects: `send_request` and `recv_request`.\n",
    "    # ----------------------------------------------------------\n",
    "    # Answer:\n",
    "\n",
    "    # =========================================================\n",
    "    # Wait for the communication to complete before proceeding.\n",
    "    send_request.wait()\n",
    "    recv_request.wait()\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # The following function test if your received tensor is correct.\n",
    "    test_func_H3T1S2(rank, world_size, message_size, received_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a8247c",
   "metadata": {
    "id": "20a8247c"
   },
   "source": [
    "**Note**: If you do not implement the function correctly, the following code will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12061b4",
   "metadata": {
    "id": "b12061b4",
    "outputId": "cc0e3a65-920b-4777-fb26-c7ad8af26c6c"
   },
   "outputs": [],
   "source": [
    "# No need to worry about this line. It's introduced to make thing go well in Jupyter notebook environment.\n",
    "from cache.H4T1S2 import send_message_to_next\n",
    "# ================================================\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "message_size = torch.Size((1, 5))\n",
    "\n",
    "# Use spawn to create separate processes for each rank\n",
    "mp.spawn(send_message_to_next, args=(world_size, message_size, backend), nprocs=world_size, join=True)\n",
    "print(\"Test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90216417",
   "metadata": {
    "id": "90216417"
   },
   "source": [
    "# Task 2: All-reduction operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f9f9cc",
   "metadata": {
    "id": "05f9f9cc"
   },
   "source": [
    "In this task, you will implement a key collective communication operation, ```all-reduce```, using only the primitive ```send``` and ```recv``` functions. Your implementation should not rely on any other collective communication functions. Let $x_n$ be a tensor on worker $n$, the result of ```all-reduce``` is that all workers have a copy of $\\sum_{n=0}^{N-1} x_n$. As we will see in Task 3, ```distributed SGD``` involves ```all-reduce``` at each iteration. Therefore, ```all-reduce``` plays a critical role in the training. You will compare the efficiency of two major methods to implement ```all-reduce```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c285054a",
   "metadata": {
    "id": "c285054a"
   },
   "source": [
    "### Step 1: Tree-all-reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54147cf0",
   "metadata": {
    "id": "54147cf0",
    "outputId": "e2279877-bcbb-4b9b-dd02-90d770d123ca"
   },
   "outputs": [],
   "source": [
    "%%writefile cache/H4T2S1.py\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import math\n",
    "\n",
    "def reduce_to_root(data, rank, world_size):\n",
    "    \"\"\"\n",
    "    Phase 1: Moves data up the binary tree to the root (Rank 0), performing reduction.\n",
    "    \"\"\"\n",
    "    # =========================================================\n",
    "    #   Step 1: Compute the number of iterations.\n",
    "    # =========================================================\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Answer:\n",
    "    iter_num = \n",
    "    # -------------------------------------------------\n",
    "\n",
    "    recv_buffer = torch.zeros_like(data)\n",
    "\n",
    "\n",
    "    for i in range(iter_num):\n",
    "\n",
    "        # =========================================================\n",
    "        #   Step 2: Determine whether this node is active or not; and if active,\n",
    "        #           determine whether it is parent (receiver) or child (sender).\n",
    "        # =========================================================\n",
    "\n",
    "        # Answer:\n",
    "\n",
    "        # =========================================================\n",
    "        #   Step 3: Send message if child; Receive and reduce if parent.\n",
    "        # =========================================================\n",
    "\n",
    "        # Answer:\n",
    "\n",
    "        \n",
    "        # -------------------------------------------------\n",
    "\n",
    "\n",
    "def broadcast_from_root(data, rank, world_size):\n",
    "    \"\"\"\n",
    "    Phase 2: Moves the final reduced result down the binary tree from the root (Rank 0).\n",
    "    \"\"\"\n",
    "\n",
    "    # =========================================================\n",
    "    #   Step 1: Compute the number of iterations.\n",
    "    # =========================================================\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Answer:\n",
    "    iter_num = \n",
    "    # -------------------------------------------------\n",
    "\n",
    "    # =========================================================\n",
    "    #   Step 2: Main loop for reduction.\n",
    "    # =========================================================\n",
    "    for i in range(iter_num):\n",
    "\n",
    "        # =========================================================\n",
    "        #   Step 3: Determine the role\n",
    "        # =========================================================\n",
    "        # Answer:\n",
    "\n",
    "        # =========================================================\n",
    "        #   Step 4: Send message if parent; Receive if children.\n",
    "        # =========================================================\n",
    "        # Answer:\n",
    "\n",
    "        \n",
    "        # -------------------------------------------------\n",
    "\n",
    "    \n",
    "def tree_all_reduce(local_data):\n",
    "    \"\"\"\n",
    "    Implements the TreeAllReduce algorithm using torch.distributed primitives.\n",
    "    \"\"\"\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "\n",
    "    if world_size == 1:\n",
    "        return local_data\n",
    "\n",
    "    \n",
    "    # =========================================================\n",
    "    # --- Phase 1: Reduce-to-Root ---\n",
    "    # =========================================================\n",
    "    # Answer:\n",
    "\n",
    "    # =========================================================\n",
    "    # --- Phase 2: Broadcast-from-Root ---\n",
    "    # =========================================================\n",
    "    # Answer:\n",
    "\n",
    "    # -------------------------------------------------\n",
    "\n",
    "    return local_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979d1ef",
   "metadata": {},
   "source": [
    "#### The following code test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ce9fea",
   "metadata": {
    "id": "c6ce9fea",
    "outputId": "30d9d3e9-b09b-40ea-9c09-99395d59a7b9"
   },
   "outputs": [],
   "source": [
    "# No need to worry about this line. It's introduced to make thing go well in Jupyter notebook environment.\n",
    "from cache.H4T2S1 import tree_all_reduce\n",
    "# ================================================\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from util import run_all_reduce\n",
    "\n",
    "message_size = torch.Size((4,))\n",
    "# Use spawn to create separate processes for each rank\n",
    "mp.spawn(run_all_reduce, args=(tree_all_reduce, world_size, message_size, backend), nprocs=world_size, join=True)\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbe50eb",
   "metadata": {
    "id": "1dbe50eb"
   },
   "source": [
    "## Step 2: Ring-All-Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d567336",
   "metadata": {
    "id": "3d567336",
    "outputId": "92a46047-c6ba-4257-a99b-f66e01c44b26"
   },
   "outputs": [],
   "source": [
    "%%writefile cache/H4T2S2.py\n",
    "\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "from util import test_func_H3T1S2, generate_random_tensor\n",
    "\n",
    "def ring_all_reduce(local_data):\n",
    "    \"\"\"\n",
    "    Implements the RingAllReduce algorithm using torch.distributed.isend and recv\n",
    "    with correct indexing for both phases.\n",
    "    \"\"\"\n",
    "\n",
    "    # =========================================================\n",
    "    #   Step 0: Initialization.\n",
    "    # =========================================================\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "\n",
    "    if world_size == 1:\n",
    "        return local_data\n",
    "\n",
    "    # A temporary tensor to store the received data\n",
    "    recv_tensor = torch.zeros_like(local_data)\n",
    "\n",
    "    # Divide the data into chunks\n",
    "    chunks = list(torch.chunk(local_data, world_size))\n",
    "\n",
    "\n",
    "    # =========================================================\n",
    "    #   Step 1: Calculate the neighbors.\n",
    "    # =========================================================\n",
    "    # Answer:\n",
    "\n",
    "    # =========================================================\n",
    "    #   Step 2: Scatter-reduce.\n",
    "    # =========================================================\n",
    "\n",
    "    for i in range(world_size - 1):\n",
    "        # Answer:\n",
    "        \n",
    "    # =========================================================\n",
    "    #   Step 3: All-Gather\n",
    "    # =========================================================\n",
    "\n",
    "    for i in range(world_size - 1):\n",
    "        # Answer:\n",
    "\n",
    "    #---------------------------------------------------------\n",
    "        \n",
    "    # Reconstruct the final tensor from the chunks\n",
    "    final_tensor = torch.cat(chunks, dim=0)\n",
    "\n",
    "    return final_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda47a9",
   "metadata": {},
   "source": [
    "#### The following code test your implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b36e39",
   "metadata": {
    "id": "a5b36e39",
    "outputId": "af981fd4-3c4f-4558-ba96-5651d725fabc"
   },
   "outputs": [],
   "source": [
    "# No need to worry about this line. It's introduced to make thing go well in Jupyter notebook environment.\n",
    "from cache.H4T2S2 import ring_all_reduce\n",
    "# ================================================\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from util import run_all_reduce\n",
    "\n",
    "message_size = torch.Size((4,))\n",
    "# Use spawn to create separate processes for each rank\n",
    "mp.spawn(run_all_reduce, args=(ring_all_reduce, world_size, message_size, backend), nprocs=world_size, join=True)\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9629f08",
   "metadata": {
    "id": "a9629f08"
   },
   "source": [
    "# Task 3: Distributed training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3b3334",
   "metadata": {
    "id": "3a3b3334"
   },
   "source": [
    "## Step 1: Stochastic gradient descent in single machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2961de05",
   "metadata": {
    "id": "2961de05"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- Define the Simple MLP Model (2 Layers) ---\n",
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-layer Multi-Layer Perceptron for MNIST (1x28x28 input).\n",
    "    Input size: 28*28 = 784\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        input_size = 28 * 28\n",
    "        hidden_size = 256 # Example hidden size\n",
    "\n",
    "        # First fully connected layer (Input 784 -> Hidden 256)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Second fully connected layer (Hidden 256 -> Output 10 classes)\n",
    "        self.fc2 = nn.Linear(hidden_size, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the 1x28x28 image into a 784 vector\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172c508",
   "metadata": {
    "id": "f172c508",
    "outputId": "2f4375e6-ba8e-488d-f6b3-a587562535f7"
   },
   "outputs": [],
   "source": [
    "%%writefile cache/H4T2S1.py\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- hyperparameters ---\n",
    "# TODO: choose appropriate hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1.0\n",
    "EPOCHS = 1\n",
    "# -----------------------\n",
    "\n",
    "def main_single_machine(model, train_dataset, test_model_func, device):\n",
    "    \"\"\"\n",
    "    Simple single-machine SGD training demonstration.\n",
    "\n",
    "    This function performs a standard synchronous mini-batch SGD training loop\n",
    "    on a single process (no distributed communication). It is meant as an\n",
    "    input demonstration showing how to use the provided arguments.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    model : torch.nn.Module\n",
    "        The model to train.\n",
    "    train_dataset : torch.utils.data.Dataset\n",
    "        Dataset providing training samples.\n",
    "    test_dataset : torch.utils.data.Dataset\n",
    "        Dataset providing test samples.\n",
    "    device : torch.device\n",
    "        Device to run training on (cpu or cuda).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of (model, loss_hist, theta_hist) where\n",
    "      'model' : The trained model.\n",
    "      'loss_hist' : (EPOCHS+1,) training loss over epochs\n",
    "      'acc_hist' : (EPOCHS+1,) test accuracy over epochs\n",
    "    \"\"\"\n",
    "\n",
    "    # Train Loader with DistributedSampler\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Optimizer, Loss\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    loss_hist, acc_hist = [], []\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # =========================================\n",
    "            #  Step 1: Forward pass and calculate loss\n",
    "            # =========================================\n",
    "            # Answer:\n",
    "\n",
    "            # =========================================\n",
    "            #  Step 2: Backward pass (calculate local gradients)\n",
    "            # =========================================\n",
    "            # Answer:\n",
    "\n",
    "            # =========================================\n",
    "            #  Step 3: Update weights using local gradients\n",
    "            # =========================================\n",
    "            # Answer:\n",
    "\n",
    "\n",
    "            # =========================================\n",
    "\n",
    "        # Record average loss\n",
    "\n",
    "\n",
    "        print(f'Epoch {epoch}: Average Loss: {total_loss / len(train_loader):.4f}, Test Accuracy: {acc:.2f}%')\n",
    "\n",
    "    return model, loss_hist, acc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4565de",
   "metadata": {
    "id": "0d4565de",
    "outputId": "0b82a498-115a-4558-b0bb-e6a782c751ba"
   },
   "outputs": [],
   "source": [
    "from cache.H4T2S1 import main_single_machine\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load Data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# --- Test Function ---\n",
    "def test_model_func(model, device):\n",
    "    \"\"\"Evaluates the model on the test dataset.\"\"\"\n",
    "    # Test Loader (only need one for rank 0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(): # Disable gradient calculations during evaluation\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # Sum up batch loss\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            # Get the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleMLP().to(device)\n",
    "main_single_machine(model, train_dataset, test_model_func, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc78ccf5",
   "metadata": {
    "id": "cc78ccf5"
   },
   "source": [
    "## Step 2: Parallel SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ed836",
   "metadata": {
    "id": "bd3ed836",
    "outputId": "f41fab79-1719-479c-e6d7-e8fa5a518345"
   },
   "outputs": [],
   "source": [
    "%%writefile cache/H4T2S2.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "# =======================================\n",
    "# --- Configuration ---\n",
    "# =======================================\n",
    "# choose appropriate hyperparameters\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1.0\n",
    "EPOCHS = 1\n",
    "\n",
    "# --- 1. Define the Simple MLP Model (2 Layers) ---\n",
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-layer Multi-Layer Perceptron for MNIST (1x28x28 input).\n",
    "    Input size: 28*28 = 784\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        input_size = 28 * 28\n",
    "        hidden_size = 256 # Example hidden size\n",
    "\n",
    "        # First fully connected layer (Input 784 -> Hidden 256)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Second fully connected layer (Hidden 256 -> Output 10 classes)\n",
    "        self.fc2 = nn.Linear(hidden_size, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the 1x28x28 image into a 784 vector\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# --- 2. Setup and Cleanup Functions for Distributed Processes ---\n",
    "def setup(rank, world_size):\n",
    "    \"\"\"Initialize the distributed environment.\"\"\"\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355' # Use a random free port\n",
    "    # Initialize the process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "    print(f\"Process {rank} successfully initialized using 'gloo' backend.\")\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Destroy the process group.\"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "# --- Test Function ---\n",
    "def test_model(model, device, test_loader):\n",
    "    \"\"\"Evaluates the model on the test dataset.\"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(): # Disable gradient calculations during evaluation\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # Sum up batch loss\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            # Get the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    test_error = 100. - accuracy\n",
    "\n",
    "    print(f'\\n======================================================')\n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)')\n",
    "    print(f'Final Test Error: {test_error:.2f}%')\n",
    "    print(f'======================================================\\n')\n",
    "\n",
    "# --- Training Worker Function (Target for mp.spawn) ---\n",
    "def run_worker(rank, world_size, result):\n",
    "    \"\"\"\n",
    "    Main function executed by each spawned process.\n",
    "    It sets up DDP, loads data, trains the model, and performs model aggregation manually.\n",
    "\n",
    "    'result' is a dictionary to store test results per epoch for rank 0 process.\n",
    "    \"\"\"\n",
    "    print(f\"Starting training on rank {rank}.\")\n",
    "\n",
    "    # 4.1 Initialize Distributed Process Group\n",
    "    setup(rank, world_size)\n",
    "    device = torch.device(\"cpu\") # For this simple example, we stick to CPU with gloo\n",
    "\n",
    "    # 4.2 Load Data (using DistributedSampler for Train, standard DataLoader for Test)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # Train Loader with DistributedSampler\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=train_sampler,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Test Loader (only need one for rank 0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "\n",
    "    # 4.3 Model, Optimizer, Loss\n",
    "    model = SimpleMLP().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 4.4 Training Loop\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # Set sampler epoch to ensure proper shuffling across epochs\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # =========================================\n",
    "            #  Step 1: Forward pass and calculate loss\n",
    "            # =========================================\n",
    "            # Answer:\n",
    "\n",
    "            # =========================================\n",
    "            #  Step 2: Backward pass (calculate local gradients)\n",
    "            # =========================================\n",
    "            # Answer:\n",
    "\n",
    "            # =========================================\n",
    "            #  Step 3: Update weights using local gradients\n",
    "            # =========================================\n",
    "            # Answer:\n",
    "\n",
    "            # =========================================\n",
    "            #  Step 4: Model Aggregation (Manual All-Reduce)\n",
    "            # =========================================\n",
    "            \n",
    "\n",
    "            for param in model.parameters():\n",
    "                # Your goal here is to:\n",
    "                # 1. Sum the parameters (param.data) across all processes.\n",
    "                # 2. Divide by the world size to get the average model parameter.\n",
    "                # Answer:\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            # =========================================\n",
    "\n",
    "        if rank == 0:\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f\"--- Rank {rank} finished Epoch {epoch}. Average Train Loss: {avg_loss:.4f} ---\")\n",
    "            ## Record test results per epoch\n",
    "            test_loss, test_acc = test_model(model, device, test_loader)\n",
    "            result[epoch] = [test_loss, test_acc]\n",
    "\n",
    "\n",
    "    # 4.6 Cleanup\n",
    "    if rank == 0:\n",
    "        print(\"Training and Evaluation complete. Cleaning up distributed environment...\")\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb32cc9",
   "metadata": {
    "id": "3cb32cc9",
    "outputId": "87284532-1aa4-4b4d-a1b2-b46f00c14061"
   },
   "outputs": [],
   "source": [
    "from cache.H4T2S2 import run_worker\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "WORLD_SIZE = 4\n",
    "## Similar to Task 1, we create a special manager dict to store results from different processes\n",
    "## We will pass this to each process so they can write their results\n",
    "manager = mp.Manager()\n",
    "result = manager.dict()\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "mp.spawn(\n",
    "    run_worker,\n",
    "    args=(WORLD_SIZE, result), # Arguments passed to run_worker: (rank, world_size)\n",
    "    nprocs=WORLD_SIZE, # Number of processes to spawn\n",
    "    join=True\n",
    ")\n",
    "\n",
    "## print the results\n",
    "for epoch, (test_loss, test_acc) in dict(result).items():\n",
    "    print(f\"Epoch {epoch}: Average Loss = {test_loss:.4f}, Test Accuracy = {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc66e63",
   "metadata": {
    "id": "4fc66e63"
   },
   "source": [
    "# Submission Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be09490e",
   "metadata": {
    "id": "be09490e"
   },
   "source": [
    "After you ensure all the codes you implement work faithfully as you want, run the following code. It will help you generate a ``hw_functions.py`` file that is ready for submission. Please submit ``hw_functions.py`` onto Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6cbe0",
   "metadata": {
    "id": "aff6cbe0",
    "outputId": "e330d627-950b-4fa1-f318-113d070d1b26"
   },
   "outputs": [],
   "source": [
    "from util import combine_cache_functions\n",
    "\n",
    "# Example usage (run this cell to create hw4_functions.py)\n",
    "combine_cache_functions(cache_dir=\"cache\", out_file=\"hw4_functions.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff1b505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
