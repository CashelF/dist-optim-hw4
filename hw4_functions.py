# Auto-generated file combining functions/classes from files in 'cache'
# Generated by combine_cache_functions

import torch
import torch.distributed as dist
from util import test_func_H3T1S2, generate_random_tensor
import torch.nn as nn
import torch.optim as optim
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import DataLoader
import os
import torch.nn.functional as F
from torchvision import datasets, transforms

def say_hello(rank, results_queue):

    """"Step 1: Distributed ''Hello World''
    """
    # =========================================
    # Answer:
    msg = f"Rank {rank}: Hello, Cash!"
    print(msg)

    if results_queue is not None:
        results_queue.put(msg)

def send_message_to_next(rank, world_size, message_size, backend="gloo"):

    # =========================================================
    #   Step 0: Initialize the process group.
    # =========================================================

    # A helper function to run the example.
    dist.init_process_group(backend=backend, rank=rank, world_size=world_size)

    # prepare the tensor to send
    my_tensor = generate_random_tensor(rank, message_size)

    # =========================================================
    #   Step 1: Create a placeholder for the received tensor.
    # =========================================================
    # The receive function requires a pre-allocated tensor
    # to place the incoming data. The size must match the data sent.
    # ----------------------------------------------------------
    # Answer:
    received_tensor = torch.zeros_like(my_tensor)

    # =========================================================
    #   Step 2: Determine sender and receiver ranks.
    # =========================================================
    # The goal is for worker `n` to send to `(n+1) % N` and
    # receive from `(n-1 + N) % N`. 
    # Your goal here is to find two values: `receiver_rank` and `source_rank`.
    # receiver_rank: The rank of the worker we will send to.
    # source_rank: The rank of the worker we will receive from.
    # ----------------------------------------------------------
    # Answer:
    receiver_rank = (rank + 1) % world_size
    source_rank = (rank - 1 + world_size) % world_size


    # =========================================================
    #   Step 3: Perform the send and receive operations.
    # =========================================================

    # We use asynchronous operations to allow for simultaneous
    # communication. This is more efficient than synchronous sends/receives.
    # `dist.isend` initiates a non-blocking send.
    # `dist.irecv` initiates a non-blocking receive.
    # Your goal here is to create two request objects: `send_request` and `recv_request`.
    # ----------------------------------------------------------
    # Answer:
    send_request = dist.isend(tensor=my_tensor, dst=receiver_rank)
    recv_request = dist.irecv(tensor=received_tensor, src=source_rank)

    # =========================================================
    # Wait for the communication to complete before proceeding.
    send_request.wait()
    recv_request.wait()

    # -------------------------------------------------
    # The following function test if your received tensor is correct.
    test_func_H3T1S2(rank, world_size, message_size, received_tensor)

def main_single_machine(model, train_dataset, test_model_func, device):
    """
    Simple single-machine SGD training demonstration.

    This function performs a standard synchronous mini-batch SGD training loop
    on a single process (no distributed communication). It is meant as an
    input demonstration showing how to use the provided arguments.

    Args
    ----
    model : torch.nn.Module
        The model to train.
    train_dataset : torch.utils.data.Dataset
        Dataset providing training samples.
    test_dataset : torch.utils.data.Dataset
        Dataset providing test samples.
    device : torch.device
        Device to run training on (cpu or cuda).

    Returns
    -------
    A tuple of (model, loss_hist, theta_hist) where
      'model' : The trained model.
      'loss_hist' : (EPOCHS+1,) training loss over epochs
      'acc_hist' : (EPOCHS+1,) test accuracy over epochs
    """

    # Train Loader with DistributedSampler
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False
    )

    # Optimizer, Loss
    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()

    loss_hist, acc_hist = [], []

    # Training Loop
    for epoch in range(1, EPOCHS + 1):
        total_loss = 0.0

        for batch_idx, (data, target) in enumerate(train_loader):

            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()

            # =========================================
            #  Step 1: Forward pass and calculate loss
            # =========================================
            # Answer:
            outputs = model(data)
            loss = criterion(outputs, target)

            # =========================================
            #  Step 2: Backward pass (calculate local gradients)
            # =========================================
            # Answer:
            loss.backward()
            # =========================================
            #  Step 3: Update weights using local gradients
            # =========================================
            # Answer:
            optimizer.step()


            # =========================================
            total_loss += loss.item()
        # Record average loss
        avg_loss = total_loss / len(train_loader)
        loss_hist.append(avg_loss)

        acc = test_model_func(model, device)
        acc_hist.append(acc)
        print(f'Epoch {epoch}: Average Loss: {total_loss / len(train_loader):.4f}, Test Accuracy: {acc:.2f}%')

    return model, loss_hist, acc_hist

class SimpleMLP(nn.Module):
    """
    Two-layer Multi-Layer Perceptron for MNIST (1x28x28 input).
    Input size: 28*28 = 784
    """
    def __init__(self):
        super(SimpleMLP, self).__init__()
        input_size = 28 * 28
        hidden_size = 256 # Example hidden size

        # First fully connected layer (Input 784 -> Hidden 256)
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        # Second fully connected layer (Hidden 256 -> Output 10 classes)
        self.fc2 = nn.Linear(hidden_size, 10)

    def forward(self, x):
        # Flatten the 1x28x28 image into a 784 vector
        x = x.view(-1, 28 * 28)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

def setup(rank, world_size):
    """Initialize the distributed environment."""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355' # Use a random free port
    # Initialize the process group
    dist.init_process_group("gloo", rank=rank, world_size=world_size)
    print(f"Process {rank} successfully initialized using 'gloo' backend.")

def cleanup():
    """Destroy the process group."""
    dist.destroy_process_group()

def test_model(model, device, test_loader):
    """Evaluates the model on the test dataset."""
    model.eval() # Set model to evaluation mode
    test_loss = 0
    correct = 0
    with torch.no_grad(): # Disable gradient calculations during evaluation
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            # Sum up batch loss
            test_loss += F.cross_entropy(output, target, reduction='sum').item()
            # Get the index of the max log-probability
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)

    accuracy = 100. * correct / len(test_loader.dataset)
    test_error = 100. - accuracy

    print(f'\n======================================================')
    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)')
    print(f'Final Test Error: {test_error:.2f}%')
    print(f'======================================================\n')

    return test_loss, accuracy

def run_worker(rank, world_size, result):
    """
    Main function executed by each spawned process.
    It sets up DDP, loads data, trains the model, and performs model aggregation manually.

    'result' is a dictionary to store test results per epoch for rank 0 process.
    """
    print(f"Starting training on rank {rank}.")

    # 4.1 Initialize Distributed Process Group
    setup(rank, world_size)
    device = torch.device("cpu") # For this simple example, we stick to CPU with gloo

    # 4.2 Load Data (using DistributedSampler for Train, standard DataLoader for Test)
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)

    # Train Loader with DistributedSampler
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        sampler=train_sampler,
        shuffle=False
    )

    # Test Loader (only need one for rank 0)
    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)


    # 4.3 Model, Optimizer, Loss
    model = SimpleMLP().to(device)
    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()

    # 4.4 Training Loop
    for epoch in range(1, EPOCHS + 1):
        # Set sampler epoch to ensure proper shuffling across epochs
        train_sampler.set_epoch(epoch)
        total_loss = 0.0

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()

            # =========================================
            #  Step 1: Forward pass and calculate loss
            # =========================================
            # Answer:
            outputs = model(data)
            loss = criterion(outputs, target)

            # =========================================
            #  Step 2: Backward pass (calculate local gradients)
            # =========================================
            # Answer:
            loss.backward()
            # =========================================
            #  Step 3: Update weights using local gradients
            # =========================================
            # Answer:
            optimizer.step()
            # =========================================
            #  Step 4: Model Aggregation (Manual All-Reduce)
            # =========================================


            for param in model.parameters():
                # Your goal here is to:
                # 1. Sum the parameters (param.data) across all processes.
                # 2. Divide by the world size to get the average model parameter.
                # Answer:
                dist.all_reduce(param.data, op=dist.ReduceOp.SUM)
                param.data /= world_size


            total_loss += loss.item()
            # =========================================

        if rank == 0:
            avg_loss = total_loss / len(train_loader)
            print(f"--- Rank {rank} finished Epoch {epoch}. Average Train Loss: {avg_loss:.4f} ---")
            ## Record test results per epoch
            test_loss, test_acc = test_model(model, device, test_loader)
            result[epoch] = [test_loss, test_acc]


    # 4.6 Cleanup
    if rank == 0:
        print("Training and Evaluation complete. Cleaning up distributed environment...")
    cleanup()

